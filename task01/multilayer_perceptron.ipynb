{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么选择的激活函数普遍具有梯度消失的特点?  \n",
    "开始的时候我一直好奇为什么选择的激活函数普遍具有梯度消失的特点，这样不就让部分神经元失活使最后结果出问题吗？后来看到一篇文章的描述才发现，正是因为模拟人脑的生物神经网络的方法。在2001年有研究表明生物脑的神经元工作具有稀疏性，这样可以节约尽可能多的能量，据研究，只有大约1%-4%的神经元被激活参与，绝大多数情况下，神经元是处于抑制状态的，因此ReLu函数反而是更加优秀的近似生物激活函数。  \n",
    "所以第一个问题，抑制现象是必须发生的，这样能更好的拟合特征。  \n",
    "那么自然也引申出了第二个问题，为什么sigmoid函数这类函数不行？  \n",
    "1.中间部分梯度值过小（最大只有0.25）因此即使在中间部分也没有办法明显的激活，反而会在多层中失活，表现非常不好。\n",
    "2.指数运算在计算中过于复杂，不利于运算，反而ReLu函数用最简单的梯度  \n",
    "在第二条解决之后，我们来看看ReLu函数所遇到的问题，  \n",
    "1.在负向部分完全失活，如果选择的超参数不好等情况，可能会出现过多神经元失活，从而整个网络死亡。  \n",
    "2.ReLu函数不是zero-centered，即激活函数输出的总是非负值，而gradient也是非负值，在back propagate情况下总会得到与输入x相同的结果，同正或者同负，因此收敛会显著受到影响，一些要减小的参数和要增加的参数会受到捆绑限制。  \n",
    "这两个问题的解决方法分别是  \n",
    "1.如果出现神经元失活的情况，可以选择调整超参数或者换成Leaky ReLu 但是，没有证据证明任何情况下都是Leaky-ReLu好  \n",
    "2.针对非zero-centered情况，可以选择用minibatch gradient decent 通过batch里面的正负调整，或者使用ELU(Exponential Linear Units)但是同样具有计算量过大的情况，同样没有证据ELU总是优于ReLU。  \n",
    "所以绝大多数情况下建议使用ReLu。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知机从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import d2lzh1981.utils as d2l\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root='D:\\\\Users\\\\Desktop\\\\haha\\\\data\\\\mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "num_hiddens = 256\n",
    "\n",
    "W1 = torch.tensor(np.random.normal(0,0.01,(num_inputs, num_hiddens)), dtype=torch.float)\n",
    "b1 = torch.zeros(num_hiddens, dtype=torch.float)\n",
    "W2 = torch.tensor(np.random.normal(0,0.01,(num_hiddens, num_outputs)), dtype=torch.float)\n",
    "b2 = torch.zeros(num_outputs, dtype=torch.float)\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "for param in  params:\n",
    "    param.requires_grad_(requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.max(input, other, out=None) \n",
    "'''\n",
    "返回两个元素的最大值。\n",
    "input(Tensor) ---- 待比较张量\n",
    "other(Tensor) ---- 比较张量\n",
    "out(Tensor,可选的) ---- 结果张量\n",
    "'''\n",
    "def relu(X):\n",
    "    return torch.max(input=X, other=torch.tensor(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.view((-1,num_inputs))\n",
    "    H = relu(torch.matmul(X,W1)+b1)\n",
    "    return torch.matmul(H,W2)+b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0054, train acc 0.605, test acc 0.605\n",
      "epoch 2, loss 0.0051, train acc 0.619, test acc 0.618\n",
      "epoch 3, loss 0.0048, train acc 0.630, test acc 0.629\n",
      "epoch 4, loss 0.0045, train acc 0.640, test acc 0.628\n",
      "epoch 5, loss 0.0043, train acc 0.646, test acc 0.637\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 0.5\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import d2lzh1981.utils as d2l\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0031, train acc 0.707, test acc 0.739\n",
      "epoch 2, loss 0.0019, train acc 0.820, test acc 0.784\n",
      "epoch 3, loss 0.0017, train acc 0.842, test acc 0.834\n",
      "epoch 4, loss 0.0015, train acc 0.857, test acc 0.821\n",
      "epoch 5, loss 0.0014, train acc 0.865, test acc 0.840\n"
     ]
    }
   ],
   "source": [
    "#初始化模型和各个参数\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "num_hiddens = 256\n",
    "batch_size = 256\n",
    "net = nn.Sequential(\n",
    "    d2l.FlattenLayer(),\n",
    "    nn.Linear(num_inputs, num_hiddens),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(num_hiddens, num_outputs),\n",
    ")\n",
    "for params in net.parameters():\n",
    "    init.normal_(params, mean=0, std=0.01)\n",
    "    \n",
    "#开始训练\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "num_epochs = 5\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
