{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本情感分类\n",
    "\n",
    "文本分类是自然语言处理的一个常见任务，它把一段不定长的文本序列变换为文本的类别。本节关注它的一个子问题：使用文本情感分类来分析文本作者的情绪。这个问题也叫情感分析，并有着广泛的应用。\n",
    "\n",
    "同搜索近义词和类比词一样，文本分类也属于词嵌入的下游应用。在本节中，我们将应用预训练的词向量和含多个隐藏层的双向循环神经网络与卷积神经网络，来判断一段不定长的文本序列中包含的是正面还是负面的情绪。后续内容将从以下几个方面展开：\n",
    "\n",
    "1. 文本情感分类数据集\n",
    "2. 使用循环神经网络进行情感分类\n",
    "3. 使用卷积神经网络进行情感分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T11:19:25.820285Z",
     "start_time": "2020-04-04T11:19:25.815322Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本情感分类数据\n",
    "\n",
    "我们使用[斯坦福的IMDb数据集（Stanford’s Large Movie Review Dataset）](http://ai.stanford.edu/~amaas/data/sentiment/)作为文本情感分类的数据集。\n",
    "\n",
    "### 读取数据\n",
    "\n",
    "数据集文件夹结构：\n",
    "\n",
    "```\n",
    "| aclImdb_v1\n",
    "    | train\n",
    "    |   | pos\n",
    "    |   |   | 0_9.txt  \n",
    "    |   |   | 1_7.txt\n",
    "    |   |   | ...\n",
    "    |   | neg\n",
    "    |   |   | 0_3.txt\n",
    "    |   |   | 1_1.txt\n",
    "    |   | ...\n",
    "    | test\n",
    "    |   | pos\n",
    "    |   | neg\n",
    "    |   | ...\n",
    "    | ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T11:19:40.877018Z",
     "start_time": "2020-04-04T11:19:25.822279Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                        | 0/12500 [00:00<?, ?it/s]\n",
      "\n",
      "  4%|██▋                                                                         | 438/12500 [00:00<00:02, 4348.21it/s]\n",
      "\n",
      "  7%|█████                                                                       | 834/12500 [00:00<00:02, 4214.13it/s]\n",
      "\n",
      " 10%|███████▏                                                                   | 1204/12500 [00:00<00:02, 4035.57it/s]\n",
      "\n",
      " 12%|█████████▏                                                                 | 1534/12500 [00:00<00:02, 3773.38it/s]\n",
      "\n",
      " 15%|███████████▏                                                               | 1856/12500 [00:00<00:02, 3579.63it/s]\n",
      "\n",
      " 17%|█████████████                                                              | 2182/12500 [00:00<00:02, 3469.03it/s]\n",
      "\n",
      " 20%|███████████████                                                            | 2505/12500 [00:00<00:02, 3386.10it/s]\n",
      "\n",
      " 23%|████████████████▉                                                          | 2820/12500 [00:00<00:02, 3304.02it/s]\n",
      "\n",
      " 25%|██████████████████▉                                                        | 3150/12500 [00:00<00:02, 3295.57it/s]\n",
      "\n",
      " 28%|████████████████████▉                                                      | 3495/12500 [00:01<00:02, 3333.12it/s]\n",
      "\n",
      " 31%|██████████████████████▉                                                    | 3818/12500 [00:01<00:02, 3264.22it/s]\n",
      "\n",
      " 33%|████████████████████████▉                                                  | 4157/12500 [00:01<00:02, 3294.19it/s]\n",
      "\n",
      " 36%|███████████████████████████                                                | 4513/12500 [00:01<00:02, 3362.52it/s]\n",
      "\n",
      " 39%|█████████████████████████████                                              | 4853/12500 [00:01<00:02, 3366.54it/s]\n",
      "\n",
      " 42%|███████████████████████████████▎                                           | 5220/12500 [00:01<00:02, 3445.08it/s]\n",
      "\n",
      " 45%|█████████████████████████████████▍                                         | 5577/12500 [00:01<00:01, 3474.00it/s]\n",
      "\n",
      " 47%|███████████████████████████████████▌                                       | 5924/12500 [00:01<00:01, 3465.40it/s]\n",
      "\n",
      " 50%|█████████████████████████████████████▋                                     | 6271/12500 [00:01<00:01, 3438.49it/s]\n",
      "\n",
      " 53%|███████████████████████████████████████▋                                   | 6624/12500 [00:01<00:01, 3458.01it/s]\n",
      "\n",
      " 56%|█████████████████████████████████████████▉                                 | 6988/12500 [00:02<00:01, 3503.46it/s]\n",
      "\n",
      " 59%|████████████████████████████████████████████                               | 7339/12500 [00:02<00:01, 3436.27it/s]\n",
      "\n",
      " 61%|██████████████████████████████████████████████                             | 7683/12500 [00:02<00:01, 3399.26it/s]\n",
      "\n",
      " 64%|████████████████████████████████████████████████▏                          | 8031/12500 [00:02<00:01, 3415.73it/s]\n",
      "\n",
      " 67%|██████████████████████████████████████████████████▎                        | 8381/12500 [00:02<00:01, 3433.11it/s]\n",
      "\n",
      " 70%|████████████████████████████████████████████████████▎                      | 8725/12500 [00:02<00:01, 3427.95it/s]\n",
      "\n",
      " 73%|██████████████████████████████████████████████████████▍                    | 9069/12500 [00:02<00:01, 3424.06it/s]\n",
      "\n",
      " 75%|████████████████████████████████████████████████████████▍                  | 9412/12500 [00:02<00:00, 3378.07it/s]\n",
      "\n",
      " 78%|██████████████████████████████████████████████████████████▌                | 9751/12500 [00:02<00:00, 3324.56it/s]\n",
      "\n",
      " 81%|███████████████████████████████████████████████████████████▋              | 10092/12500 [00:02<00:00, 3342.54it/s]\n",
      "\n",
      " 84%|█████████████████████████████████████████████████████████████▊            | 10438/12500 [00:03<00:00, 3369.67it/s]\n",
      "\n",
      " 86%|███████████████████████████████████████████████████████████████▊          | 10776/12500 [00:03<00:00, 3365.44it/s]\n",
      "\n",
      " 89%|█████████████████████████████████████████████████████████████████▊        | 11116/12500 [00:03<00:00, 3368.34it/s]\n",
      "\n",
      " 92%|███████████████████████████████████████████████████████████████████▊      | 11459/12500 [00:03<00:00, 3379.37it/s]\n",
      "\n",
      " 94%|█████████████████████████████████████████████████████████████████████▊    | 11798/12500 [00:03<00:00, 3365.36it/s]\n",
      "\n",
      " 97%|███████████████████████████████████████████████████████████████████████▊  | 12135/12500 [00:03<00:00, 3319.78it/s]\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:03<00:00, 3395.65it/s]\n",
      "\n",
      "\n",
      "  0%|                                                                                        | 0/12500 [00:00<?, ?it/s]\n",
      "\n",
      "  2%|█▊                                                                          | 300/12500 [00:00<00:04, 2978.20it/s]\n",
      "\n",
      "  5%|███▌                                                                        | 582/12500 [00:00<00:04, 2922.27it/s]\n",
      "\n",
      "  7%|█████▍                                                                      | 899/12500 [00:00<00:03, 2986.24it/s]\n",
      "\n",
      " 10%|███████▍                                                                   | 1247/12500 [00:00<00:03, 3112.89it/s]\n",
      "\n",
      " 13%|█████████▌                                                                 | 1597/12500 [00:00<00:03, 3213.08it/s]\n",
      "\n",
      " 16%|███████████▋                                                               | 1942/12500 [00:00<00:03, 3273.84it/s]\n",
      "\n",
      " 18%|█████████████▋                                                             | 2290/12500 [00:00<00:03, 3326.10it/s]\n",
      "\n",
      " 21%|███████████████▊                                                           | 2641/12500 [00:00<00:02, 3372.28it/s]\n",
      "\n",
      " 24%|█████████████████▉                                                         | 2983/12500 [00:00<00:02, 3378.93it/s]\n",
      "\n",
      " 27%|███████████████████▉                                                       | 3330/12500 [00:01<00:02, 3398.62it/s]\n",
      "\n",
      " 29%|█████████████████████▉                                                     | 3663/12500 [00:01<00:02, 3349.96it/s]\n",
      "\n",
      " 32%|███████████████████████▉                                                   | 3993/12500 [00:01<00:02, 3327.44it/s]\n",
      "\n",
      " 35%|██████████████████████████                                                 | 4353/12500 [00:01<00:02, 3397.56it/s]\n",
      "\n",
      " 38%|████████████████████████████▎                                              | 4725/12500 [00:01<00:02, 3481.03it/s]\n",
      "\n",
      " 41%|██████████████████████████████▌                                            | 5085/12500 [00:01<00:02, 3508.39it/s]\n",
      "\n",
      " 44%|████████████████████████████████▋                                          | 5445/12500 [00:01<00:01, 3527.82it/s]\n",
      "\n",
      " 46%|██████████████████████████████████▊                                        | 5804/12500 [00:01<00:01, 3538.55it/s]\n",
      "\n",
      " 49%|████████████████████████████████████▉                                      | 6158/12500 [00:01<00:01, 3520.74it/s]\n",
      "\n",
      " 52%|███████████████████████████████████████                                    | 6510/12500 [00:01<00:01, 3481.64it/s]\n",
      "\n",
      " 55%|█████████████████████████████████████████▏                                 | 6874/12500 [00:02<00:01, 3520.27it/s]\n",
      "\n",
      " 58%|███████████████████████████████████████████▎                               | 7227/12500 [00:02<00:01, 3453.67it/s]\n",
      "\n",
      " 61%|█████████████████████████████████████████████▍                             | 7573/12500 [00:02<00:01, 3427.64it/s]\n",
      "\n",
      " 63%|███████████████████████████████████████████████▌                           | 7917/12500 [00:02<00:01, 3393.73it/s]\n",
      "\n",
      " 66%|█████████████████████████████████████████████████▌                         | 8257/12500 [00:02<00:01, 3368.13it/s]\n",
      "\n",
      " 69%|███████████████████████████████████████████████████▌                       | 8595/12500 [00:02<00:01, 3305.25it/s]\n",
      "\n",
      " 71%|█████████████████████████████████████████████████████▌                     | 8926/12500 [00:02<00:01, 3270.05it/s]\n",
      "\n",
      " 74%|███████████████████████████████████████████████████████▌                   | 9254/12500 [00:02<00:00, 3246.68it/s]\n",
      "\n",
      " 77%|█████████████████████████████████████████████████████████▍                 | 9579/12500 [00:02<00:00, 3211.84it/s]\n",
      "\n",
      " 79%|███████████████████████████████████████████████████████████▌               | 9926/12500 [00:02<00:00, 3278.12it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████████████████████████████████▊             | 10281/12500 [00:03<00:00, 3348.51it/s]\n",
      "\n",
      " 85%|██████████████████████████████████████████████████████████████▉           | 10641/12500 [00:03<00:00, 3412.84it/s]\n",
      "\n",
      " 88%|█████████████████████████████████████████████████████████████████         | 10989/12500 [00:03<00:00, 3425.36it/s]\n",
      "\n",
      " 91%|███████████████████████████████████████████████████████████████████▏      | 11353/12500 [00:03<00:00, 3479.75it/s]\n",
      "\n",
      " 94%|█████████████████████████████████████████████████████████████████████▍    | 11721/12500 [00:03<00:00, 3530.06it/s]\n",
      "\n",
      " 97%|███████████████████████████████████████████████████████████████████████▌  | 12078/12500 [00:03<00:00, 3534.26it/s]\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:03<00:00, 3400.28it/s]\n",
      "\n",
      "\n",
      "  0%|                                                                                        | 0/12500 [00:00<?, ?it/s]\n",
      "\n",
      "  3%|██▏                                                                         | 369/12500 [00:00<00:03, 3663.28it/s]\n",
      "\n",
      "  6%|████▎                                                                       | 700/12500 [00:00<00:03, 3541.29it/s]\n",
      "\n",
      "  8%|██████▏                                                                    | 1038/12500 [00:00<00:03, 3483.12it/s]\n",
      "\n",
      " 11%|████████▎                                                                  | 1379/12500 [00:00<00:03, 3453.17it/s]\n",
      "\n",
      " 14%|██████████▎                                                                | 1725/12500 [00:00<00:03, 3447.95it/s]\n",
      "\n",
      " 17%|████████████▍                                                              | 2072/12500 [00:00<00:03, 3447.02it/s]\n",
      "\n",
      " 19%|██████████████▌                                                            | 2432/12500 [00:00<00:02, 3483.86it/s]\n",
      "\n",
      " 22%|████████████████▋                                                          | 2773/12500 [00:00<00:02, 3453.76it/s]\n",
      "\n",
      " 25%|██████████████████▋                                                        | 3117/12500 [00:00<00:02, 3442.26it/s]\n",
      "\n",
      " 28%|████████████████████▋                                                      | 3447/12500 [00:01<00:02, 3390.65it/s]\n",
      "\n",
      " 30%|██████████████████████▋                                                    | 3777/12500 [00:01<00:02, 3345.05it/s]\n",
      "\n",
      " 33%|████████████████████████▋                                                  | 4105/12500 [00:01<00:02, 3308.09it/s]\n",
      "\n",
      " 35%|██████████████████████████▌                                                | 4432/12500 [00:01<00:02, 3279.24it/s]\n",
      "\n",
      " 38%|████████████████████████████▌                                              | 4757/12500 [00:01<00:02, 3234.26it/s]\n",
      "\n",
      " 41%|██████████████████████████████▍                                            | 5079/12500 [00:01<00:02, 3203.71it/s]\n",
      "\n",
      " 43%|████████████████████████████████▍                                          | 5399/12500 [00:01<00:02, 3195.56it/s]\n",
      "\n",
      " 46%|██████████████████████████████████▎                                        | 5726/12500 [00:01<00:02, 3210.43it/s]\n",
      "\n",
      " 48%|████████████████████████████████████▎                                      | 6057/12500 [00:01<00:01, 3232.91it/s]\n",
      "\n",
      " 51%|██████████████████████████████████████▍                                    | 6407/12500 [00:01<00:01, 3301.82it/s]\n",
      "\n",
      " 54%|████████████████████████████████████████▍                                  | 6749/12500 [00:02<00:01, 3329.04it/s]\n",
      "\n",
      " 57%|██████████████████████████████████████████▌                                | 7088/12500 [00:02<00:01, 3340.12it/s]\n",
      "\n",
      " 59%|████████████████████████████████████████████▌                              | 7427/12500 [00:02<00:01, 3347.42it/s]\n",
      "\n",
      " 62%|██████████████████████████████████████████████▌                            | 7762/12500 [00:02<00:01, 3341.13it/s]\n",
      "\n",
      " 65%|████████████████████████████████████████████████▋                          | 8105/12500 [00:02<00:01, 3359.80it/s]\n",
      "\n",
      " 68%|██████████████████████████████████████████████████▋                        | 8456/12500 [00:02<00:01, 3396.54it/s]\n",
      "\n",
      " 70%|████████████████████████████████████████████████████▊                      | 8796/12500 [00:02<00:01, 3370.04it/s]\n",
      "\n",
      " 73%|██████████████████████████████████████████████████████▊                    | 9134/12500 [00:02<00:01, 3355.67it/s]\n",
      "\n",
      " 76%|████████████████████████████████████████████████████████▊                  | 9470/12500 [00:02<00:00, 3290.73it/s]\n",
      "\n",
      " 78%|██████████████████████████████████████████████████████████▊                | 9800/12500 [00:02<00:00, 3219.19it/s]\n",
      "\n",
      " 81%|███████████████████████████████████████████████████████████▉              | 10123/12500 [00:03<00:00, 3177.52it/s]\n",
      "\n",
      " 84%|█████████████████████████████████████████████████████████████▊            | 10442/12500 [00:03<00:00, 3174.08it/s]\n",
      "\n",
      " 86%|███████████████████████████████████████████████████████████████▊          | 10771/12500 [00:03<00:00, 3201.20it/s]\n",
      "\n",
      " 89%|█████████████████████████████████████████████████████████████████▋        | 11093/12500 [00:03<00:00, 3199.78it/s]\n",
      "\n",
      " 91%|███████████████████████████████████████████████████████████████████▋      | 11430/12500 [00:03<00:00, 3242.23it/s]\n",
      "\n",
      " 94%|█████████████████████████████████████████████████████████████████████▋    | 11776/12500 [00:03<00:00, 3297.90it/s]\n",
      "\n",
      " 97%|███████████████████████████████████████████████████████████████████████▊  | 12141/12500 [00:03<00:00, 3389.09it/s]\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:03<00:00, 3325.40it/s]\n",
      "\n",
      "\n",
      "  0%|                                                                                        | 0/12500 [00:00<?, ?it/s]\n",
      "\n",
      "  3%|██▏                                                                         | 358/12500 [00:00<00:03, 3553.37it/s]\n",
      "\n",
      "  6%|████▏                                                                       | 693/12500 [00:00<00:03, 3481.86it/s]\n",
      "\n",
      "  8%|██████                                                                     | 1015/12500 [00:00<00:03, 3391.03it/s]\n",
      "\n",
      " 11%|████████                                                                   | 1345/12500 [00:00<00:03, 3355.96it/s]\n",
      "\n",
      " 13%|██████████                                                                 | 1675/12500 [00:00<00:03, 3331.60it/s]\n",
      "\n",
      " 16%|████████████                                                               | 2014/12500 [00:00<00:03, 3341.67it/s]\n",
      "\n",
      " 19%|██████████████                                                             | 2348/12500 [00:00<00:03, 3333.71it/s]\n",
      "\n",
      " 22%|████████████████▏                                                          | 2695/12500 [00:00<00:02, 3366.17it/s]\n",
      "\n",
      " 24%|██████████████████▏                                                        | 3030/12500 [00:00<00:02, 3353.92it/s]\n",
      "\n",
      " 27%|████████████████████▏                                                      | 3361/12500 [00:01<00:02, 3333.23it/s]\n",
      "\n",
      " 30%|██████████████████████▏                                                    | 3708/12500 [00:01<00:02, 3366.02it/s]\n",
      "\n",
      " 32%|████████████████████████▎                                                  | 4045/12500 [00:01<00:02, 3359.80it/s]\n",
      "\n",
      " 35%|██████████████████████████▎                                                | 4387/12500 [00:01<00:02, 3370.41it/s]\n",
      "\n",
      " 38%|████████████████████████████▎                                              | 4725/12500 [00:01<00:02, 3365.92it/s]\n",
      "\n",
      " 41%|██████████████████████████████▍                                            | 5075/12500 [00:01<00:02, 3397.80it/s]\n",
      "\n",
      " 43%|████████████████████████████████▌                                          | 5425/12500 [00:01<00:02, 3420.49it/s]\n",
      "\n",
      " 46%|██████████████████████████████████▋                                        | 5787/12500 [00:01<00:01, 3470.68it/s]\n",
      "\n",
      " 49%|████████████████████████████████████▊                                      | 6134/12500 [00:01<00:01, 3422.23it/s]\n",
      "\n",
      " 52%|██████████████████████████████████████▊                                    | 6476/12500 [00:01<00:01, 3353.76it/s]\n",
      "\n",
      " 54%|████████████████████████████████████████▊                                  | 6812/12500 [00:02<00:01, 3309.04it/s]\n",
      "\n",
      " 57%|██████████████████████████████████████████▊                                | 7144/12500 [00:02<00:01, 3275.61it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████▊                              | 7472/12500 [00:02<00:01, 3269.76it/s]\n",
      "\n",
      " 62%|██████████████████████████████████████████████▊                            | 7800/12500 [00:02<00:01, 3246.36it/s]\n",
      "\n",
      " 65%|████████████████████████████████████████████████▊                          | 8125/12500 [00:02<00:01, 3155.56it/s]\n",
      "\n",
      " 68%|██████████████████████████████████████████████████▊                        | 8461/12500 [00:02<00:01, 3207.76it/s]\n",
      "\n",
      " 71%|████████████████████████████████████████████████████▉                      | 8821/12500 [00:02<00:01, 3309.29it/s]\n",
      "\n",
      " 73%|███████████████████████████████████████████████████████                    | 9172/12500 [00:02<00:00, 3359.93it/s]\n",
      "\n",
      " 76%|█████████████████████████████████████████████████████████                  | 9515/12500 [00:02<00:00, 3373.42it/s]\n",
      "\n",
      " 79%|███████████████████████████████████████████████████████████▏               | 9856/12500 [00:02<00:00, 3377.16it/s]\n",
      "\n",
      " 82%|████████████████████████████████████████████████████████████▍             | 10200/12500 [00:03<00:00, 3388.44it/s]\n",
      "\n",
      " 84%|██████████████████████████████████████████████████████████████▍           | 10540/12500 [00:03<00:00, 3374.46it/s]\n",
      "\n",
      " 87%|████████████████████████████████████████████████████████████████▍         | 10878/12500 [00:03<00:00, 3233.74it/s]\n",
      "\n",
      " 90%|██████████████████████████████████████████████████████████████████▎       | 11203/12500 [00:03<00:00, 3183.97it/s]\n",
      "\n",
      " 92%|████████████████████████████████████████████████████████████████████▏     | 11523/12500 [00:03<00:00, 3153.68it/s]\n",
      "\n",
      " 95%|██████████████████████████████████████████████████████████████████████▏   | 11853/12500 [00:03<00:00, 3189.60it/s]\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:03<00:00, 3322.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t the film starts with a voice over telling the audi\n",
      "0 \t in the glory days of the 90s (god rest its soul) y\n",
      "1 \t my favorite part of this film was the old man's at\n",
      "0 \t in the future of 2001, freddy is after the last su\n",
      "0 \t blazing saddles! it's a fight between two estrange\n"
     ]
    }
   ],
   "source": [
    "def read_imdb(folder='train', data_root=\"/home/kesci/input/IMDB2578/aclImdb_v1/aclImdb\"):\n",
    "    data = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_name = os.path.join(data_root, folder, label)\n",
    "        for file in tqdm(os.listdir(folder_name)):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '').lower()\n",
    "                data.append([review, 1 if label == 'pos' else 0])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "DATA_ROOT = \"../../data/IMDB2578/aclImdb_v1/\"\n",
    "data_root = os.path.join(DATA_ROOT, \"aclImdb\")\n",
    "train_data, test_data = read_imdb('train', data_root), read_imdb('test', data_root)\n",
    "\n",
    "# 打印训练数据中的前五个sample\n",
    "for sample in train_data[:5]:\n",
    "    print(sample[1], '\\t', sample[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T11:19:40.885996Z",
     "start_time": "2020-04-04T11:19:40.880042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the film starts with a voice over telling the audience where they are, and who the characters are. and that is the moment i started to dislike the movie. with all the endless possibilities any film director have in hand, i really find it a very easy and cheap solution to express the situation with a voice over telling everything. i actually believe voice overs are betrayals to the film making concept.<br /><br />i hate to hear from a voice over saying where we are, which date we are at, and especially what the characters feel and think. i believe that a director has to find a visual way to transmit the feelings and the thoughts of the characters to the audience. <br /><br />but after the bad influencing intro, a very striking movie begins and keeps going for a fairly long enough time. the lives of a middle class family and all the members individually are depicted in a perfect realistic way. i think the director has a talent for capturing real life situations. for example, a father who has to make his private calls from the bathroom might seem abnormal at first, but life itself leads us some situations which might seem abnormal but also very normal as well. i think the director is a very good observer about real life.<br /><br />but that is it. after a while the realism in the movie begins to sacrifice the story-telling. i really felt like i\\'m having a big headache because of the non-stop talking characters. it was as if the actors and actresses were given the subject and were allowed to improvise the dialogs. it is realistic really, but characters always asking \"really, is that so\" etc. to each other, or characters saying \"no\" or \"are you listening to me,\" ten times when saying it only once is just enough causes me to have a headache.<br /><br />i also think the play practicing and book reading scenes are more then they should be. i understand that the play and the book in the movie are very much related to the plot, but i think the director has missed the point where he should stop showing these scenes.', 0]\n"
     ]
    }
   ],
   "source": [
    "for sample in train_data[:1]:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理数据\n",
    "\n",
    "读取数据后，我们先根据文本的格式进行单词的切分，再利用 [`torchtext.vocab.Vocab`](https://torchtext.readthedocs.io/en/latest/vocab.html#vocab) 创建词典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T11:19:45.041905Z",
     "start_time": "2020-04-04T11:19:40.889984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words in vocab: 46152\n"
     ]
    }
   ],
   "source": [
    "def get_tokenized_imdb(data):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 数据的列表，列表中的每个元素为 [文本字符串，0/1标签] 二元组\n",
    "    @return: 切分词后的文本的列表，列表中的每个元素为切分后的词序列\n",
    "    '''\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(' ')]\n",
    "    \n",
    "    return [tokenizer(review) for review, _ in data]\n",
    "\n",
    "def get_vocab_imdb(data):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 同上\n",
    "    @return: 数据集上的词典，Vocab 的实例（freqs, stoi, itos）\n",
    "    '''\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter, min_freq=5)\n",
    "\n",
    "vocab = get_vocab_imdb(train_data)\n",
    "print('# words in vocab:', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T09:31:46.013924Z",
     "start_time": "2020-04-04T09:31:46.005946Z"
    }
   },
   "source": [
    "词典和词语的索引创建好后，就可以将数据集的文本从字符串的形式转换为单词下标序列的形式，以待之后的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T11:19:45.051879Z",
     "start_time": "2020-04-04T11:19:45.043900Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_imdb(data, vocab):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 同上，原始的读入数据\n",
    "        vocab: 训练集上生成的词典\n",
    "    @return:\n",
    "        features: 单词下标序列，形状为 (n, max_l) 的整数张量\n",
    "        labels: 情感标签，形状为 (n,) 的0/1整数张量\n",
    "    '''\n",
    "    max_l = 500  # 将每条评论通过截断或者补0，使得长度变成500\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = get_tokenized_imdb(data)\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建数据迭代器\n",
    "\n",
    "利用 [`torch.utils.data.TensorDataset`](https://pytorch.org/docs/stable/data.html?highlight=tensor%20dataset#torch.utils.data.TensorDataset)，可以创建 PyTorch 格式的数据集，从而创建数据迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T11:19:53.377587Z",
     "start_time": "2020-04-04T11:19:45.054846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([64, 500]) y torch.Size([64])\n",
      "#batches: 391\n"
     ]
    }
   ],
   "source": [
    "train_set = Data.TensorDataset(*preprocess_imdb(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess_imdb(test_data, vocab))\n",
    "\n",
    "# 上面的代码等价于下面的注释代码\n",
    "# train_features, train_labels = preprocess_imdb(train_data, vocab)\n",
    "# test_features, test_labels = preprocess_imdb(test_data, vocab)\n",
    "# train_set = Data.TensorDataset(train_features, train_labels)\n",
    "# test_set = Data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "# len(train_set) = features.shape[0] or labels.shape[0]\n",
    "# train_set[index] = (features[index], labels[index])\n",
    "\n",
    "batch_size = 64\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)\n",
    "\n",
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "print('#batches:', len(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用循环神经网络\n",
    "\n",
    "### 双向循环神经网络\n",
    "\n",
    "在[“双向循环神经网络”](https://zh.d2l.ai/chapter_recurrent-neural-networks/bi-rnn.html)一节中，我们介绍了其模型与前向计算的公式，这里简单回顾一下：\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5mnobct47.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5mo6okdnp.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "\n",
    "给定输入序列 $\\{\\boldsymbol{X}_1,\\boldsymbol{X}_2,\\dots,\\boldsymbol{X}_T\\}$，其中 $\\boldsymbol{X}_t\\in\\mathbb{R}^{n\\times d}$ 为时间步（批量大小为 $n$，输入维度为 $d$）。在双向循环神经网络的架构中，设时间步 $t$ 上的正向隐藏状态为 $\\overrightarrow{\\boldsymbol{H}}_{t} \\in \\mathbb{R}^{n \\times h}$ （正向隐藏状态维度为 $h$），反向隐藏状态为 $\\overleftarrow{\\boldsymbol{H}}_{t} \\in \\mathbb{R}^{n \\times h}$ （反向隐藏状态维度为 $h$）。我们可以分别计算正向隐藏状态和反向隐藏状态：\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\overrightarrow{\\boldsymbol{H}}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}^{(f)}+\\overrightarrow{\\boldsymbol{H}}_{t-1} \\boldsymbol{W}_{h h}^{(f)}+\\boldsymbol{b}_{h}^{(f)}\\right)\\\\\n",
    "&\\overleftarrow{\\boldsymbol{H}}_{t}=\\phi\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}^{(b)}+\\overleftarrow{\\boldsymbol{H}}_{t+1} \\boldsymbol{W}_{h h}^{(b)}+\\boldsymbol{b}_{h}^{(b)}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "其中权重 $\\boldsymbol{W}_{x h}^{(f)} \\in \\mathbb{R}^{d \\times h}, \\boldsymbol{W}_{h h}^{(f)} \\in \\mathbb{R}^{h \\times h}, \\boldsymbol{W}_{x h}^{(b)} \\in \\mathbb{R}^{d \\times h}, \\boldsymbol{W}_{h h}^{(b)} \\in \\mathbb{R}^{h \\times h}$ 和偏差 $\\boldsymbol{b}_{h}^{(f)} \\in \\mathbb{R}^{1 \\times h}, \\boldsymbol{b}_{h}^{(b)} \\in \\mathbb{R}^{1 \\times h}$ 均为模型参数，$\\phi$ 为隐藏层激活函数。\n",
    "\n",
    "然后我们连结两个方向的隐藏状态 $\\overrightarrow{\\boldsymbol{H}}_{t}$ 和 $\\overleftarrow{\\boldsymbol{H}}_{t}$ 来得到隐藏状态 $\\boldsymbol{H}_{t} \\in \\mathbb{R}^{n \\times 2 h}$，并将其输入到输出层。输出层计算输出 $\\boldsymbol{O}_{t} \\in \\mathbb{R}^{n \\times q}$（输出维度为 $q$）：\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{O}_{t}=\\boldsymbol{H}_{t} \\boldsymbol{W}_{h q}+\\boldsymbol{b}_{q}\n",
    "$$\n",
    "\n",
    "\n",
    "其中权重 $\\boldsymbol{W}_{h q} \\in \\mathbb{R}^{2 h \\times q}$ 和偏差 $\\boldsymbol{b}_{q} \\in \\mathbb{R}^{1 \\times q}$ 为输出层的模型参数。不同方向上的隐藏单元维度也可以不同。\n",
    "\n",
    "利用 [`torch.nn.RNN`](https://pytorch.org/docs/stable/nn.html?highlight=rnn#torch.nn.RNN) 或 [`torch.nn.LSTM`](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM) 模组，我们可以很方便地实现双向循环神经网络，下面是以 LSTM 为例的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T10:43:00.517926Z",
     "start_time": "2020-04-04T10:43:00.414170Z"
    }
   },
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n",
    "        '''\n",
    "        @params:\n",
    "            vocab: 在数据集上创建的词典，用于获取词典大小\n",
    "            embed_size: 嵌入维度大小\n",
    "            num_hiddens: 隐藏状态维度大小\n",
    "            num_layers: 隐藏层个数\n",
    "        '''\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        \n",
    "        # encoder-decoder framework\n",
    "        # bidirectional设为True即得到双向循环神经网络\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, \n",
    "                                hidden_size=num_hiddens, \n",
    "                                num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        self.decoder = nn.Linear(4*num_hiddens, 2) # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        @params:\n",
    "            inputs: 词语下标序列，形状为 (batch_size, seq_len) 的整数张量\n",
    "        @return:\n",
    "            outs: 对文本情感的预测，形状为 (batch_size, 2) 的张量\n",
    "        '''\n",
    "        # 因为LSTM需要将序列长度(seq_len)作为第一维，所以需要将输入转置\n",
    "        embeddings = self.embedding(inputs.permute(1, 0)) # (seq_len, batch_size, d)\n",
    "        # rnn.LSTM 返回输出、隐藏状态和记忆单元，格式如 outputs, (h, c)\n",
    "        outputs, _ = self.encoder(embeddings) # (seq_len, batch_size, 2*h)\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1) # (batch_size, 4*h)\n",
    "        outs = self.decoder(encoding) # (batch_size, 2)\n",
    "        return outs\n",
    "\n",
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载预训练的词向量\n",
    "\n",
    "由于预训练词向量的词典及词语索引与我们使用的数据集并不相同，所以需要根据目前的词典及索引的顺序来加载预训练词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T11:19:15.445810Z",
     "start_time": "2020-04-04T11:18:59.851250Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                       | 0/140318 [00:00<?, ?it/s]\n",
      "  1%|▍                                                                          | 722/140318 [00:00<00:19, 7167.70it/s]\n",
      "  1%|▊                                                                         | 1445/140318 [00:00<00:19, 7170.60it/s]\n",
      "  2%|█▏                                                                        | 2165/140318 [00:00<00:19, 7163.30it/s]\n",
      "  2%|█▌                                                                        | 2947/140318 [00:00<00:18, 7333.36it/s]\n",
      "  3%|██                                                                        | 3838/140318 [00:00<00:17, 7729.61it/s]\n",
      "  4%|██▌                                                                       | 4912/140318 [00:00<00:16, 8425.30it/s]\n",
      "  4%|███▏                                                                      | 6092/140318 [00:00<00:14, 9199.73it/s]\n",
      "  5%|███▊                                                                      | 7245/140318 [00:00<00:13, 9775.92it/s]\n",
      "  6%|████▎                                                                    | 8362/140318 [00:00<00:13, 10135.49it/s]\n",
      "  7%|████▉                                                                    | 9483/140318 [00:01<00:12, 10414.33it/s]\n",
      "  8%|█████▍                                                                  | 10583/140318 [00:01<00:12, 10561.66it/s]\n",
      "  8%|█████▉                                                                  | 11641/140318 [00:01<00:12, 10512.72it/s]\n",
      "  9%|██████▌                                                                 | 12694/140318 [00:01<00:12, 10463.73it/s]\n",
      " 10%|███████                                                                 | 13742/140318 [00:01<00:12, 10322.62it/s]\n",
      " 11%|███████▌                                                                | 14776/140318 [00:01<00:12, 10065.17it/s]\n",
      " 11%|████████▏                                                                | 15785/140318 [00:01<00:12, 9961.50it/s]\n",
      " 12%|████████▋                                                                | 16783/140318 [00:01<00:12, 9915.71it/s]\n",
      " 13%|█████████▏                                                               | 17780/140318 [00:01<00:12, 9910.29it/s]\n",
      " 13%|█████████▊                                                               | 18772/140318 [00:01<00:12, 9832.91it/s]\n",
      " 14%|██████████▎                                                              | 19776/140318 [00:02<00:12, 9872.76it/s]\n",
      " 15%|██████████▊                                                              | 20780/140318 [00:02<00:12, 9900.97it/s]\n",
      " 16%|███████████▏                                                            | 21846/140318 [00:02<00:11, 10096.05it/s]\n",
      " 16%|███████████▋                                                            | 22872/140318 [00:02<00:11, 10122.12it/s]\n",
      " 17%|████████████▎                                                           | 23928/140318 [00:02<00:11, 10227.91it/s]\n",
      " 18%|████████████▊                                                           | 25044/140318 [00:02<00:11, 10469.76it/s]\n",
      " 19%|█████████████▍                                                          | 26148/140318 [00:02<00:10, 10611.33it/s]\n",
      " 19%|█████████████▉                                                          | 27212/140318 [00:02<00:10, 10534.79it/s]\n",
      " 20%|██████████████▌                                                         | 28267/140318 [00:02<00:10, 10391.54it/s]\n",
      " 21%|███████████████                                                         | 29333/140318 [00:02<00:10, 10448.14it/s]\n",
      " 22%|███████████████▌                                                        | 30399/140318 [00:03<00:10, 10488.17it/s]\n",
      " 22%|████████████████▏                                                       | 31469/140318 [00:03<00:10, 10528.06it/s]\n",
      " 23%|████████████████▋                                                       | 32553/140318 [00:03<00:10, 10597.20it/s]\n",
      " 24%|█████████████████▎                                                      | 33669/140318 [00:03<00:09, 10737.07it/s]\n",
      " 25%|█████████████████▊                                                      | 34744/140318 [00:03<00:09, 10685.49it/s]\n",
      " 26%|██████████████████▍                                                     | 35829/140318 [00:03<00:09, 10711.21it/s]\n",
      " 26%|██████████████████▉                                                     | 36901/140318 [00:03<00:09, 10595.77it/s]\n",
      " 27%|███████████████████▍                                                    | 37962/140318 [00:03<00:09, 10483.71it/s]\n",
      " 28%|████████████████████                                                    | 39012/140318 [00:03<00:09, 10403.65it/s]\n",
      " 29%|████████████████████▌                                                   | 40091/140318 [00:03<00:09, 10493.36it/s]\n",
      " 29%|█████████████████████                                                   | 41141/140318 [00:04<00:09, 10380.25it/s]\n",
      " 30%|█████████████████████▋                                                  | 42180/140318 [00:04<00:09, 10147.77it/s]\n",
      " 31%|██████████████████████▍                                                  | 43197/140318 [00:04<00:09, 9925.94it/s]\n",
      " 31%|██████████████████████▉                                                  | 44197/140318 [00:04<00:09, 9925.81it/s]\n",
      " 32%|███████████████████████▌                                                 | 45213/140318 [00:04<00:09, 9973.41it/s]\n",
      " 33%|████████████████████████                                                 | 46222/140318 [00:04<00:09, 9986.22it/s]\n",
      " 34%|████████████████████████▌                                                | 47223/140318 [00:04<00:09, 9971.71it/s]\n",
      " 34%|████████████████████████▊                                               | 48239/140318 [00:04<00:09, 10006.40it/s]\n",
      " 35%|█████████████████████████▎                                              | 49251/140318 [00:04<00:09, 10017.81it/s]\n",
      " 36%|█████████████████████████▊                                              | 50265/140318 [00:04<00:08, 10032.95it/s]\n",
      " 37%|██████████████████████████▎                                             | 51311/140318 [00:05<00:08, 10135.05it/s]\n",
      " 37%|██████████████████████████▉                                             | 52376/140318 [00:05<00:08, 10262.69it/s]\n",
      " 38%|███████████████████████████▍                                            | 53415/140318 [00:05<00:08, 10278.83it/s]\n",
      " 39%|███████████████████████████▉                                            | 54444/140318 [00:05<00:08, 10198.23it/s]\n",
      " 40%|████████████████████████████▍                                           | 55465/140318 [00:05<00:08, 10089.13it/s]\n",
      " 40%|████████████████████████████▉                                           | 56483/140318 [00:05<00:08, 10094.90it/s]\n",
      " 41%|█████████████████████████████▌                                          | 57532/140318 [00:05<00:08, 10187.92it/s]\n",
      " 42%|██████████████████████████████                                          | 58561/140318 [00:05<00:08, 10196.13it/s]\n",
      " 42%|██████████████████████████████▌                                         | 59581/140318 [00:05<00:07, 10174.99it/s]\n",
      " 43%|███████████████████████████████                                         | 60657/140318 [00:06<00:07, 10321.98it/s]\n",
      " 44%|███████████████████████████████▋                                        | 61724/140318 [00:06<00:07, 10402.32it/s]\n",
      " 45%|████████████████████████████████▏                                       | 62847/140318 [00:06<00:07, 10614.89it/s]\n",
      " 46%|████████████████████████████████▊                                       | 63983/140318 [00:06<00:07, 10805.34it/s]\n",
      " 46%|█████████████████████████████████▍                                      | 65101/140318 [00:06<00:06, 10891.80it/s]\n",
      " 47%|█████████████████████████████████▉                                      | 66247/140318 [00:06<00:06, 11032.94it/s]\n",
      " 48%|██████████████████████████████████▌                                     | 67352/140318 [00:06<00:06, 11014.56it/s]\n",
      " 49%|███████████████████████████████████▏                                    | 68455/140318 [00:06<00:06, 10897.58it/s]\n",
      " 50%|███████████████████████████████████▋                                    | 69546/140318 [00:06<00:06, 10845.16it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████▏                                   | 70632/140318 [00:06<00:06, 10825.90it/s]\n",
      " 51%|████████████████████████████████████▊                                   | 71766/140318 [00:07<00:06, 10951.94it/s]\n",
      " 52%|█████████████████████████████████████▍                                  | 72862/140318 [00:07<00:06, 10833.43it/s]\n",
      " 53%|█████████████████████████████████████▉                                  | 73964/140318 [00:07<00:06, 10864.36it/s]\n",
      " 53%|██████████████████████████████████████▌                                 | 75052/140318 [00:07<00:06, 10718.26it/s]\n",
      " 54%|███████████████████████████████████████                                 | 76125/140318 [00:07<00:06, 10602.65it/s]\n",
      " 55%|███████████████████████████████████████▌                                | 77187/140318 [00:07<00:06, 10460.74it/s]\n",
      " 56%|████████████████████████████████████████▏                               | 78235/140318 [00:07<00:06, 10319.91it/s]\n",
      " 56%|████████████████████████████████████████▋                               | 79269/140318 [00:07<00:05, 10242.20it/s]\n",
      " 57%|█████████████████████████████████████████▏                              | 80295/140318 [00:07<00:05, 10195.44it/s]\n",
      " 58%|█████████████████████████████████████████▋                              | 81364/140318 [00:07<00:05, 10317.08it/s]\n",
      " 59%|██████████████████████████████████████████▎                             | 82450/140318 [00:08<00:05, 10452.07it/s]\n",
      " 60%|██████████████████████████████████████████▉                             | 83562/140318 [00:08<00:05, 10621.59it/s]\n",
      " 60%|███████████████████████████████████████████▍                            | 84637/140318 [00:08<00:05, 10636.71it/s]\n",
      " 61%|███████████████████████████████████████████▉                            | 85702/140318 [00:08<00:05, 10617.45it/s]\n",
      " 62%|████████████████████████████████████████████▌                           | 86765/140318 [00:08<00:05, 10442.23it/s]\n",
      " 63%|█████████████████████████████████████████████                           | 87811/140318 [00:08<00:05, 10332.30it/s]\n",
      " 63%|█████████████████████████████████████████████▌                          | 88846/140318 [00:08<00:05, 10223.54it/s]\n",
      " 64%|██████████████████████████████████████████████▏                         | 89923/140318 [00:08<00:04, 10358.93it/s]\n",
      " 65%|██████████████████████████████████████████████▋                         | 91028/140318 [00:08<00:04, 10535.14it/s]\n",
      " 66%|███████████████████████████████████████████████▎                        | 92155/140318 [00:08<00:04, 10723.50it/s]\n",
      " 66%|███████████████████████████████████████████████▊                        | 93238/140318 [00:09<00:04, 10731.89it/s]\n",
      " 67%|████████████████████████████████████████████████▍                       | 94313/140318 [00:09<00:04, 10712.91it/s]\n",
      " 68%|████████████████████████████████████████████████▉                       | 95432/140318 [00:09<00:04, 10829.63it/s]\n",
      " 69%|█████████████████████████████████████████████████▌                      | 96517/140318 [00:09<00:04, 10811.41it/s]\n",
      " 70%|██████████████████████████████████████████████████                      | 97621/140318 [00:09<00:03, 10855.52it/s]\n",
      " 70%|██████████████████████████████████████████████████▋                     | 98729/140318 [00:09<00:03, 10899.04it/s]\n",
      " 71%|███████████████████████████████████████████████████▏                    | 99857/140318 [00:09<00:03, 10987.08it/s]\n",
      " 72%|███████████████████████████████████████████████████                    | 100983/140318 [00:09<00:03, 11043.76it/s]\n",
      " 73%|███████████████████████████████████████████████████▋                   | 102088/140318 [00:09<00:03, 11021.47it/s]\n",
      " 74%|████████████████████████████████████████████████████▏                  | 103191/140318 [00:09<00:03, 10774.26it/s]\n",
      " 74%|████████████████████████████████████████████████████▊                  | 104270/140318 [00:10<00:03, 10444.50it/s]\n",
      " 75%|█████████████████████████████████████████████████████▎                 | 105318/140318 [00:10<00:03, 10308.81it/s]\n",
      " 76%|█████████████████████████████████████████████████████▊                 | 106352/140318 [00:10<00:03, 10234.47it/s]\n",
      " 77%|██████████████████████████████████████████████████████▎                | 107378/140318 [00:10<00:03, 10189.51it/s]\n",
      " 77%|██████████████████████████████████████████████████████▊                | 108439/140318 [00:10<00:03, 10290.06it/s]\n",
      " 78%|███████████████████████████████████████████████████████▍               | 109518/140318 [00:10<00:02, 10413.05it/s]\n",
      " 79%|███████████████████████████████████████████████████████▉               | 110601/140318 [00:10<00:02, 10513.23it/s]\n",
      " 80%|████████████████████████████████████████████████████████▌              | 111690/140318 [00:10<00:02, 10600.04it/s]\n",
      " 80%|█████████████████████████████████████████████████████████              | 112895/140318 [00:10<00:02, 10975.05it/s]\n",
      " 81%|█████████████████████████████████████████████████████████▋             | 113997/140318 [00:11<00:02, 10835.68it/s]\n",
      " 82%|██████████████████████████████████████████████████████████▎            | 115126/140318 [00:11<00:02, 10944.83it/s]\n",
      " 83%|██████████████████████████████████████████████████████████▊            | 116261/140318 [00:11<00:02, 11039.52it/s]\n",
      " 84%|███████████████████████████████████████████████████████████▍           | 117418/140318 [00:11<00:02, 11169.75it/s]\n",
      " 84%|███████████████████████████████████████████████████████████▉           | 118539/140318 [00:11<00:01, 11157.66it/s]\n",
      " 85%|████████████████████████████████████████████████████████████▌          | 119661/140318 [00:11<00:01, 11151.72it/s]\n",
      " 86%|█████████████████████████████████████████████████████████████          | 120784/140318 [00:11<00:01, 11151.61it/s]\n",
      " 87%|█████████████████████████████████████████████████████████████▋         | 121941/140318 [00:11<00:01, 11249.26it/s]\n",
      " 88%|██████████████████████████████████████████████████████████████▎        | 123068/140318 [00:11<00:01, 11231.51it/s]\n",
      " 89%|██████████████████████████████████████████████████████████████▊        | 124192/140318 [00:11<00:01, 11175.51it/s]\n",
      " 89%|███████████████████████████████████████████████████████████████▍       | 125310/140318 [00:12<00:01, 10956.19it/s]\n",
      " 90%|███████████████████████████████████████████████████████████████▉       | 126407/140318 [00:12<00:01, 10807.45it/s]\n",
      " 91%|████████████████████████████████████████████████████████████████▌      | 127490/140318 [00:12<00:01, 10790.32it/s]\n",
      " 92%|█████████████████████████████████████████████████████████████████      | 128570/140318 [00:12<00:01, 10643.54it/s]\n",
      " 92%|█████████████████████████████████████████████████████████████████▌     | 129636/140318 [00:12<00:01, 10469.07it/s]\n",
      " 93%|██████████████████████████████████████████████████████████████████▏    | 130692/140318 [00:12<00:00, 10472.57it/s]\n",
      " 94%|██████████████████████████████████████████████████████████████████▋    | 131746/140318 [00:12<00:00, 10469.82it/s]\n",
      " 95%|███████████████████████████████████████████████████████████████████▏   | 132835/140318 [00:12<00:00, 10569.93it/s]\n",
      " 95%|███████████████████████████████████████████████████████████████████▊   | 133924/140318 [00:12<00:00, 10641.93it/s]\n",
      " 96%|████████████████████████████████████████████████████████████████████▎  | 134989/140318 [00:12<00:00, 10464.95it/s]\n",
      " 97%|████████████████████████████████████████████████████████████████████▊  | 136037/140318 [00:13<00:00, 10384.59it/s]\n",
      " 98%|█████████████████████████████████████████████████████████████████████▎ | 137077/140318 [00:13<00:00, 10153.97it/s]\n",
      " 98%|█████████████████████████████████████████████████████████████████████▉ | 138095/140318 [00:13<00:00, 10139.61it/s]\n",
      " 99%|██████████████████████████████████████████████████████████████████████▍| 139176/140318 [00:13<00:00, 10310.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████▉| 140243/140318 [00:13<00:00, 10393.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1077,  0.1105,  0.5981, -0.5436,  0.6740,  0.1066,  0.0389,  0.3548,\n",
      "         0.0635, -0.0942,  0.1579, -0.8166,  0.1417,  0.2194,  0.5850, -0.5216,\n",
      "         0.2278, -0.1664, -0.6823,  0.3587,  0.4257,  0.1902,  0.9196,  0.5756,\n",
      "         0.4618,  0.4236, -0.0954, -0.4275, -0.1657, -0.0568, -0.2959,  0.2604,\n",
      "        -0.2661, -0.0704, -0.2766,  0.1582,  0.6982,  0.4308,  0.2795, -0.4544,\n",
      "        -0.3380, -0.5818,  0.2236, -0.5778, -0.2686, -0.2042,  0.5639, -0.5852,\n",
      "        -0.1436, -0.6422,  0.0055, -0.3525,  0.1616,  1.1796, -0.4767, -2.7553,\n",
      "        -0.1321, -0.0477,  1.0655,  1.1034, -0.2208,  0.1867,  0.1318,  0.1512,\n",
      "         0.7131, -0.3521,  0.9135,  0.6178,  0.7099,  0.2395, -0.1457, -0.3786,\n",
      "        -0.0460, -0.4737,  0.2385,  0.2054, -0.1900,  0.3251, -1.1112, -0.3634,\n",
      "         0.9868, -0.0848, -0.5401,  0.1173, -1.0194, -0.2442,  0.1277,  0.0139,\n",
      "         0.0804, -0.3541,  0.3495, -0.7226,  0.3755,  0.4441, -0.9906,  0.6121,\n",
      "        -0.3511, -0.8316,  0.4529,  0.0826])\n",
      "There are 22245 oov words.\n"
     ]
    }
   ],
   "source": [
    "cache_dir = \"../../data/GloVe6B5429\"\n",
    "# with open(\"../../data/GloVe6B5429/glove.6B.50d.txt\", encoding='utf-8') as f:\n",
    "#     for i in f.readlines():\n",
    "#         a = (len(i.split(' ')))\n",
    "#         if a != 101:\n",
    "            \n",
    "#             print(i)\n",
    "        \n",
    "# import sys\n",
    "# sys.exit(-1)\n",
    "glove_vocab = Vocab.GloVe(name='6B', dim=100, cache=cache_dir)\n",
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    print(pretrained_vocab.vectors[1])\n",
    "    '''\n",
    "    @params:\n",
    "        words: 需要加载词向量的词语列表，以 itos (index to string) 的词典形式给出\n",
    "        pretrained_vocab: 预训练词向量\n",
    "    @return:\n",
    "        embed: 加载到的词向量\n",
    "    '''\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    return embed\n",
    "\n",
    "net.embedding.weight.data.copy_(load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.embedding.weight.requires_grad = False # 直接加载预训练好的, 所以不需要更新它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T12:05:13.765238Z",
     "start_time": "2020-04-04T12:05:13.752272Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval()\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train()\n",
    "            else:\n",
    "                if('is_training' in net.__code__.co_varnames):\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y) \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T12:11:00.962147Z",
     "start_time": "2020-04-04T12:05:18.368297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.5836, train acc 0.667, test acc 0.822, time 74.6 sec\n",
      "epoch 2, loss 0.1817, train acc 0.841, test acc 0.856, time 67.2 sec\n",
      "epoch 3, loss 0.1042, train acc 0.867, test acc 0.865, time 63.5 sec\n",
      "epoch 4, loss 0.0689, train acc 0.886, test acc 0.869, time 65.3 sec\n",
      "epoch 5, loss 0.0483, train acc 0.903, test acc 0.863, time 64.5 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评价模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T12:14:23.229819Z",
     "start_time": "2020-04-04T12:14:23.202886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_sentiment(net, vocab, sentence):\n",
    "    '''\n",
    "    @params：\n",
    "        net: 训练好的模型\n",
    "        vocab: 在该数据集上创建的词典，用于将给定的单词序转换为单词下标的序列，从而输入模型\n",
    "        sentence: 需要分析情感的文本，以单词序列的形式给出\n",
    "    @return: 预测的结果，positive 为正面情绪文本，negative 为负面情绪文本\n",
    "    '''\n",
    "    device = list(net.parameters())[0].device # 读取模型所在的环境\n",
    "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n",
    "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "    return 'positive' if label.item() == 1 else 'negative'\n",
    "\n",
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T12:15:29.536245Z",
     "start_time": "2020-04-04T12:15:29.512282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用卷积神经网络\n",
    "\n",
    "### 一维卷积层\n",
    "\n",
    "在介绍模型前我们先来解释一维卷积层的工作原理。与二维卷积层一样，一维卷积层使用一维的互相关运算。在一维互相关运算中，卷积窗口从输入数组的最左方开始，按从左往右的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。如图所示，输入是一个宽为 7 的一维数组，核数组的宽为 2。可以看到输出的宽度为 7−2+1=6，且第一个元素是由输入的最左边的宽为 2 的子数组与核数组按元素相乘后再相加得到的：0×1+1×2=2。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5mo8qs7dc.png?imageView2/0/w/960/h/960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T12:48:30.612362Z",
     "start_time": "2020-04-04T12:48:30.603409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  5.,  8., 11., 14., 17.])\n"
     ]
    }
   ],
   "source": [
    "def corr1d(X, K):\n",
    "    '''\n",
    "    @params:\n",
    "        X: 输入，形状为 (seq_len,) 的张量\n",
    "        K: 卷积核，形状为 (w,) 的张量\n",
    "    @return:\n",
    "        Y: 输出，形状为 (seq_len - w + 1,) 的张量\n",
    "    '''\n",
    "    w = K.shape[0] # 卷积窗口宽度\n",
    "    Y = torch.zeros((X.shape[0] - w + 1))\n",
    "    for i in range(Y.shape[0]): # 滑动窗口\n",
    "        Y[i] = (X[i: i + w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "X, K = torch.tensor([0, 1, 2, 3, 4, 5, 6]), torch.tensor([1, 2])\n",
    "print(corr1d(X, K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多输入通道的一维互相关运算也与多输入通道的二维互相关运算类似：在每个通道上，将核与相应的输入做一维互相关运算，并将通道之间的结果相加得到输出结果。下图展示了含 3 个输入通道的一维互相关运算，其中阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：0×1+1×2+1×3+2×4+2×(−1)+3×(−3)=2。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5moaawczv.png?imageView2/0/w/960/h/960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T12:49:08.933989Z",
     "start_time": "2020-04-04T12:49:08.923993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  8., 14., 20., 26., 32.])\n"
     ]
    }
   ],
   "source": [
    "def corr1d_multi_in(X, K):\n",
    "    # 首先沿着X和K的通道维遍历并计算一维互相关结果。然后将所有结果堆叠起来沿第0维累加\n",
    "    return torch.stack([corr1d(x, k) for x, k in zip(X, K)]).sum(dim=0)\n",
    "    # [corr1d(X[i], K[i]) for i in range(X.shape[0])]\n",
    "\n",
    "X = torch.tensor([[0, 1, 2, 3, 4, 5, 6],\n",
    "              [1, 2, 3, 4, 5, 6, 7],\n",
    "              [2, 3, 4, 5, 6, 7, 8]])\n",
    "K = torch.tensor([[1, 2], [3, 4], [-1, -3]])\n",
    "print(corr1d_multi_in(X, K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由二维互相关运算的定义可知，多输入通道的一维互相关运算可以看作单输入通道的二维互相关运算。如图所示，我们也可以将图中多输入通道的一维互相关运算以等价的单输入通道的二维互相关运算呈现。这里核的高等于输入的高。图中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：2×(−1)+3×(−3)+1×3+2×4+0×1+1×2=2。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5moav6ue3.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "*注：反之仅当二维卷积核的高度等于输入的高度时才成立。*\n",
    "\n",
    "之前的例子中输出都只有一个通道。我们在[“多输入通道和多输出通道”](https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html)一节中介绍了如何在二维卷积层中指定多个输出通道。类似地，我们也可以在一维卷积层指定多个输出通道，从而拓展卷积层中的模型参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 时序最大池化层\n",
    "\n",
    "类似地，我们有一维池化层。TextCNN 中使用的时序最大池化（max-over-time pooling）层实际上对应一维全局最大池化层：假设输入包含多个通道，各通道由不同时间步上的数值组成，各通道的输出即该通道所有时间步中最大的数值。因此，时序最大池化层的输入在各个通道上的时间步数可以不同。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5mobv3kol.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "*注：自然语言中还有一些其他的池化操作，可参考这篇[博文](https://blog.csdn.net/malefactor/article/details/51078135)。*\n",
    "\n",
    "为提升计算性能，我们常常将不同长度的时序样本组成一个小批量，并通过在较短序列后附加特殊字符（如0）令批量中各时序样本长度相同。这些人为添加的特殊字符当然是无意义的。由于时序最大池化的主要目的是抓取时序中最重要的特征，它通常能使模型不受人为添加字符的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:24:28.220742Z",
     "start_time": "2020-04-04T13:24:28.216752Z"
    }
   },
   "outputs": [],
   "source": [
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        @params:\n",
    "            x: 输入，形状为 (batch_size, n_channels, seq_len) 的张量\n",
    "        @return: 时序最大池化后的结果，形状为 (batch_size, n_channels, 1) 的张量\n",
    "        '''\n",
    "        return F.max_pool1d(x, kernel_size=x.shape[2]) # kenerl_size=seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextCNN 模型\n",
    "\n",
    "TextCNN 模型主要使用了一维卷积层和时序最大池化层。假设输入的文本序列由 $n$ 个词组成，每个词用 $d$ 维的词向量表示。那么输入样本的宽为 $n$，输入通道数为 $d$。TextCNN 的计算主要分为以下几步。\n",
    "\n",
    "1. 定义多个一维卷积核，并使用这些卷积核对输入分别做卷积计算。宽度不同的卷积核可能会捕捉到不同个数的相邻词的相关性。\n",
    "2. 对输出的所有通道分别做时序最大池化，再将这些通道的池化输出值连结为向量。\n",
    "3. 通过全连接层将连结后的向量变换为有关各类别的输出。这一步可以使用丢弃层应对过拟合。\n",
    "\n",
    "下图用一个例子解释了 TextCNN 的设计。这里的输入是一个有 11 个词的句子，每个词用 6 维词向量表示。因此输入序列的宽为 11，输入通道数为 6。给定 2 个一维卷积核，核宽分别为 2 和 4，输出通道数分别设为 4 和 5。因此，一维卷积计算后，4 个输出通道的宽为 11−2+1=10，而其他 5 个通道的宽为 11−4+1=8。尽管每个通道的宽不同，我们依然可以对各个通道做时序最大池化，并将 9 个通道的池化输出连结成一个 9 维向量。最终，使用全连接将 9 维向量变换为 2 维输出，即正面情感和负面情感的预测。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5modgownf.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "下面我们来实现 TextCNN 模型。与上一节相比，除了用一维卷积层替换循环神经网络外，这里我们还使用了两个嵌入层，一个的权重固定，另一个则参与训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:25:06.978804Z",
     "start_time": "2020-04-04T13:25:06.819231Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, kernel_sizes, num_channels):\n",
    "        '''\n",
    "        @params:\n",
    "            vocab: 在数据集上创建的词典，用于获取词典大小\n",
    "            embed_size: 嵌入维度大小\n",
    "            kernel_sizes: 卷积核大小列表\n",
    "            num_channels: 卷积通道数列表\n",
    "        '''\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size) # 参与训练的嵌入层\n",
    "        self.constant_embedding = nn.Embedding(len(vocab), embed_size) # 不参与训练的嵌入层\n",
    "        \n",
    "        self.pool = GlobalMaxPool1d() # 时序最大池化层没有权重，所以可以共用一个实例\n",
    "        self.convs = nn.ModuleList()  # 创建多个一维卷积层\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(in_channels = 2*embed_size, \n",
    "                                        out_channels = c, \n",
    "                                        kernel_size = k))\n",
    "            \n",
    "        self.decoder = nn.Linear(sum(num_channels), 2)\n",
    "        self.dropout = nn.Dropout(0.5) # 丢弃层用于防止过拟合\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        @params:\n",
    "            inputs: 词语下标序列，形状为 (batch_size, seq_len) 的整数张量\n",
    "        @return:\n",
    "            outputs: 对文本情感的预测，形状为 (batch_size, 2) 的张量\n",
    "        '''\n",
    "        embeddings = torch.cat((\n",
    "            self.embedding(inputs), \n",
    "            self.constant_embedding(inputs)), dim=2) # (batch_size, seq_len, 2*embed_size)\n",
    "        # 根据一维卷积层要求的输入格式，需要将张量进行转置\n",
    "        embeddings = embeddings.permute(0, 2, 1) # (batch_size, 2*embed_size, seq_len)\n",
    "        \n",
    "        encoding = torch.cat([\n",
    "            self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        # encoding = []\n",
    "        # for conv in self.convs:\n",
    "        #     out = conv(embeddings) # (batch_size, out_channels, seq_len-kernel_size+1)\n",
    "        #     out = self.pool(F.relu(out)) # (batch_size, out_channels, 1)\n",
    "        #     encoding.append(out.squeeze(-1)) # (batch_size, out_channels)\n",
    "        # encoding = torch.cat(encoding) # (batch_size, out_channels_sum)\n",
    "        \n",
    "        # 应用丢弃法后使用全连接层得到输出\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs\n",
    "\n",
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "net = TextCNN(vocab, embed_size, kernel_sizes, nums_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练并评价模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:27:47.550164Z",
     "start_time": "2020-04-04T13:25:22.241732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.6370, train acc 0.662, test acc 0.770, time 29.0 sec\n",
      "epoch 2, loss 0.2460, train acc 0.763, test acc 0.824, time 29.0 sec\n",
      "epoch 3, loss 0.1357, train acc 0.811, test acc 0.840, time 29.0 sec\n",
      "epoch 4, loss 0.0822, train acc 0.855, test acc 0.858, time 29.1 sec\n",
      "epoch 5, loss 0.0492, train acc 0.902, test acc 0.866, time 29.1 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:27:57.487541Z",
     "start_time": "2020-04-04T13:27:57.478564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'great'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
