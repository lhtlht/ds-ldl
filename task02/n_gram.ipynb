{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语言模型  \n",
    "gram模型的缺点：  \n",
    "参数系数，参数空间过大，存储和内存吃不消  \n",
    "没有考虑文本中出现过多的废话词，可以用tf-idf优化  \n",
    "没有考虑久远之前出现的词语对现在的影响，可以用bigram、trigram等词袋模型解决  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63281\n",
      "想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和你融化在一起\n",
      "融化在宇宙里\n",
      "我每天每天每\n"
     ]
    }
   ],
   "source": [
    "with open('./jaychou_lyrics.txt', encoding='utf-8') as f: # win环境下加个编码encoding\n",
    "    corpus_chars = f.read()\n",
    "print(len(corpus_chars))\n",
    "print(corpus_chars[0:40])\n",
    "corpus_chars = corpus_chars.replace('\\n',' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[0:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立字符索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1027\n",
      "\n",
      "char: 想-要-有-直-升-机- -想-要-和-你-飞-到-宇-宙-去- -想-要-和\n",
      "sample: [913, 799, 783, 724, 369, 935, 399, 913, 799, 108, 639, 468, 1006, 585, 284, 181, 399, 913, 799, 108]\n"
     ]
    }
   ],
   "source": [
    "idx_to_char = list(set(corpus_chars)) #得到所有的去重字符\n",
    "char_to_idx = {char:i for i,char in enumerate(idx_to_char)} # 字典形式，每个字符对应一个唯一索引\n",
    "vocab_size = len(char_to_idx)\n",
    "print(vocab_size)\n",
    "\n",
    "corpus_indices = [char_to_idx[char] for char in corpus_chars] #语料集里每个字符对应的索引集合\n",
    "sample = corpus_indices[0:20]\n",
    "print()\n",
    "a = '-'.join([idx_to_char[idx] for idx in sample])\n",
    "print(f'char: {a}')\n",
    "print(f'sample: {sample}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_jay_lyrics():\n",
    "    with open('./jaychou_lyrics.txt', encoding='utf-8') as f: # win环境下加个编码encoding\n",
    "        corpus_chars = f.read()\n",
    "    corpus_chars = corpus_chars.replace('\\n',' ').replace('\\r', ' ')\n",
    "    corpus_chars = corpus_chars[0:10000]\n",
    "    idx_to_char = list(set(corpus_chars))\n",
    "    char_to_idx = {char:i for i,char in enumerate(idx_to_char)}\n",
    "    vocab_size = len(char_to_idx)\n",
    "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "    return corpus_indices, char_to_idx, idx_to_char, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时序数据的采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练中我们需要每次随机读取小批量样本和标签。与之前章节的实验数据不同的是，时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。该样本的标签序列为这些字符分别在训练集中的下一个字符，即“要”“有”“直”“升”“机”，即 X =“想要有直升”， Y =“要有直升机”。  \n",
    "\n",
    "现在我们考虑序列“想要有直升机，想要和你飞到宇宙去”，如果时间步数为5，有以下可能的样本和标签：  \n",
    "\n",
    "X ：“想要有直升”， Y ：“要有直升机”   \n",
    "X ：“要有直升机”， Y ：“有直升机，”  \n",
    "X ：“有直升机，”， Y ：“直升机，想”  \n",
    "...  \n",
    "X ：“要和你飞到”， Y ：“和你飞到宇”  \n",
    "X ：“和你飞到宇”， Y ：“你飞到宇宙”  \n",
    "X ：“你飞到宇宙”， Y ：“飞到宇宙去”  \n",
    "可以看到，如果序列的长度为 T ，时间步数为 n ，那么一共有 T−n 个合法的样本，但是这些样本有大量的重合，我们通常采用更加高效的采样方式。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机采样  \n",
    "下面的代码每次从数据里随机采样一个小批量。其中批量大小batch_size是每个小批量的样本数，num_steps是每个样本所包含的时间步数。 在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。引用下别人的图片帮助自己理解。   \n",
    "![jupyter](./采样.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps, device=None):\n",
    "    # 减1是因为对于长度为n的序列，X最多只有包含其中的前n - 1个字符\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps  # 下取整，得到不重叠情况下的样本个数\n",
    "    example_indices = [i * num_steps for i in range(num_examples)]  # 每个样本的第一个字符在corpus_indices中的下标\n",
    "    random.shuffle(example_indices)\n",
    "\n",
    "    def _data(i):\n",
    "        # 返回从i开始的长为num_steps的序列\n",
    "        return corpus_indices[i: i + num_steps]\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        # 每次选出batch_size个随机样本\n",
    "        batch_indices = example_indices[i: i + batch_size]  # 当前batch的各个样本的首字符的下标\n",
    "        X = [_data(j) for j in batch_indices]\n",
    "        Y = [_data(j + 1) for j in batch_indices]\n",
    "        yield torch.tensor(X, device=device), torch.tensor(Y, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[12, 13, 14, 15, 16, 17],\n",
      "        [ 0,  1,  2,  3,  4,  5]]) \n",
      "Y: tensor([[13, 14, 15, 16, 17, 18],\n",
      "        [ 1,  2,  3,  4,  5,  6]]) \n",
      "\n",
      "X:  tensor([[ 6,  7,  8,  9, 10, 11],\n",
      "        [18, 19, 20, 21, 22, 23]]) \n",
      "Y: tensor([[ 7,  8,  9, 10, 11, 12],\n",
      "        [19, 20, 21, 22, 23, 24]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(30))\n",
    "for X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相邻采样  \n",
    "在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    corpus_len = len(corpus_indices) // batch_size * batch_size  # 保留下来的序列的长度\n",
    "    corpus_indices = corpus_indices[: corpus_len]  # 仅保留前corpus_len个字符\n",
    "    indices = torch.tensor(corpus_indices, device=device)\n",
    "    indices = indices.view(batch_size, -1)  # resize成(batch_size, )\n",
    "    batch_num = (indices.shape[1] - 1) // num_steps #计算每一块最多能取多少样本\n",
    "    for i in range(batch_num):\n",
    "        i = i * num_steps #避免不重复\n",
    "        X = indices[:, i: i + num_steps]\n",
    "        Y = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "b = a.view(3, -1)\n",
    "print(b.shape)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [15, 16, 17, 18, 19, 20]]) \n",
      "Y: tensor([[ 1,  2,  3,  4,  5,  6],\n",
      "        [16, 17, 18, 19, 20, 21]]) \n",
      "\n",
      "X:  tensor([[ 6,  7,  8,  9, 10, 11],\n",
      "        [21, 22, 23, 24, 25, 26]]) \n",
      "Y: tensor([[ 7,  8,  9, 10, 11, 12],\n",
      "        [22, 23, 24, 25, 26, 27]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
